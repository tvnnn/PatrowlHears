from django.core.management.base import BaseCommand
# from common.feeds.metadata import import_exploit
from common.utils import chunks
from celery import group
from data.models import DataFeedImport
from data.tasks import import_exploit_task
import time
import os
import re
import json
from tqdm import tqdm
from datetime import datetime

CHUNK_SIZE = 32
CHECKSUMS_FILENAME = 'checksums.md5'


class Command(BaseCommand):
    help = 'Import exploits from Feeds JSON files'

    def add_arguments(self, parser):
        parser.add_argument('-d', '--input-dir', type=str, help='Input directory', )
        parser.add_argument('-y', '--year', type=str, help='CVEs from Year',)
        parser.add_argument('-l', '--last-update', type=str, help='Last update date (YYYY-MM-DD)',)
        parser.add_argument('-f', '--feeds', type=str, help='Feeds list - comma-separated (Default: all)',)
        parser.add_argument('-e', '--enumerate-feeds', action='store_true', help='Just enumerate feeds list',)

    def handle(self, *args, **options):
        start_time = time.time()
        input_dir = options['input_dir']
        last_update = None
        allowed_feeds = None

        # Validate options
        if options['input_dir'] in [None, ''] or os.path.isdir(input_dir) is False:
            print("Unable to locate directory. Abort captain. ABORT !")
            return

        if options['enumerate_feeds'] is True:
            for feed_dirname in os.listdir(input_dir):
                feed_dirname_full = os.path.join(input_dir, feed_dirname, 'data/exploits')
                if os.path.isdir(feed_dirname_full) and feed_dirname not in ['__pycache__']:
                    print(feed_dirname)
            return

        if options['feeds'] not in [None, '']:
            try:
                allowed_feeds = options['feeds'].split(',')
            except Exception:
                print("Bad feeds format (use commas). Abort captain. ABORT !")
                return

        if options['last_update'] not in [None, '']:
            last_update = ""
            try:
                last_update = datetime.strptime(options['last_update'], '%Y-%m-%d')
                print("Only last updates from {}".format(last_update))
            except Exception:
                print("Bad datetime format (Use 'YYYY-MM-DD' instead). Abort captain. ABORT !")
                return

        print("Importing data from: '{}'".format(input_dir))

        # Find exploits
        for feed_dirname in os.listdir(input_dir):
            if allowed_feeds is not None and feed_dirname not in allowed_feeds:
                continue

            print("Checking already submitted files")
            feed_checksums = get_checksums(input_dir, feed_dirname)
            already_imported_files = list(DataFeedImport.objects.filter(hash__in=feed_checksums.keys(), has_error=False, type='exploit', source=feed_dirname.lower()).values_list('filename', flat=True))

            feed_dirname_full = os.path.join(input_dir, feed_dirname)
            feed_datadir = os.path.join(feed_dirname_full, 'data')
            feed_data_exploits_dir = os.path.join(feed_datadir, 'exploits')
            if os.path.isdir(feed_dirname_full) and os.path.isdir(feed_datadir) and os.path.isdir(feed_data_exploits_dir):
                feed_files = []  # filenames
                feed_files_sig = []  # task signatures
                for year_dir in os.listdir(feed_data_exploits_dir):
                    if options['year'] not in [None, ''] and re.match(r'.*([1-3][0-9]{3})', options['year']) is not None and options['year'] != year_dir.split('/')[-1]:
                        continue
                    year_dir_path = os.path.join(feed_data_exploits_dir, year_dir)
                    for month_dir in os.listdir(year_dir_path):
                        month_dir_path = os.path.join(year_dir_path, month_dir)
                        for day_dir in os.listdir(month_dir_path):
                            day_dir_path = os.path.join(month_dir_path, day_dir)
                            for exploit_filename in os.listdir(day_dir_path):
                                if exploit_filename not in already_imported_files:
                                    feed_files.append(os.path.join(day_dir_path, exploit_filename))

                for file in tqdm(feed_files, desc="{}-pre".format(feed_dirname)):
                    try:
                        with open(file, 'r') as f:
                            file_data = json.load(f)
                            file_checked_at = datetime.strptime(file_data['checked_at'].split(' ')[0], '%Y-%m-%d')
                            if last_update in [None, ''] or last_update < file_checked_at:
                                # import_exploit(file_data)
                                # import_exploit_task.apply_async(
                                #     args=[file_data],
                                #     queue='data',
                                #     retry=False
                                # )
                                feed_files_sig.append(import_exploit_task.s(file_data).set(queue='data'))

                    except Exception as e:
                        print(e)

                pbar = tqdm(total=len(feed_files_sig), desc="{}-run".format(feed_dirname))
                for chunk in chunks(feed_files_sig, CHUNK_SIZE):
                    res = group(chunk)()
                    res.get()
                    pbar.update(CHUNK_SIZE)
                pbar.close()

        elapsed_time = time.time() - start_time
        print("Import done! Well done captain.")
        print("Elapsed time (in seconds): %.3f" % elapsed_time)


def get_checksums(input_dir, feed_dirname):
    chks = {}
    with open(input_dir+CHECKSUMS_FILENAME, 'r') as f:
        for line in f.readlines():
            hash = line.split(' ')[0]
            filename = line.split(' ')[2].replace('\n', '')
            if feed_dirname+'/data/exploits/' in filename:
                chks.update({hash: filename})
    return chks
